# Домашнее задание к занятию «Микросервисы: подходы»

Формат: Markdown для копирования в файл `.md`.

---

## Задача 1: Обеспечить разработку (SCM + CI/CD)

### Требования
- Облачная система.
- Git, репозиторий на каждый сервис.
- Запуск сборки по событию из VCS.
- Запуск сборки вручную с параметрами.
- Настройки на уровне сборки/пайплайна.
- Шаблоны для различных конфигураций сборок.
- Безопасное хранение секретов.
- Несколько конфигураций сборки из одного репозитория.
- Кастомные шаги сборки.
- Собственные Docker-образы для сборки.
- Возможность развернуть self-hosted build agents на своих серверах.
- Параллельный запуск сборок и тестов.

### Предложенное решение
Стек:
- Git-репозитории: GitLab (SaaS) или GitHub (SaaS).
- CI/CD: GitLab CI/CD (встроено) или GitHub Actions.
- Self-hosted агенты: GitLab Runner или GitHub Self-hosted runners.
- Хранение секретов: встроенные секреты CI (Masked/Protected variables, Environments secrets) плюс опционально внешний Vault.
- Registry артефактов/образов: встроенный Container Registry (GitLab/GitHub) либо отдельный (Harbor).

Выбор (для выполнения требований с минимальной сложностью):
- GitLab SaaS + GitLab CI/CD + GitLab Runner.

### Как взаимодействуют компоненты
1) Каждый микросервис имеет отдельный Git-репозиторий.
2) В репозитории хранится декларативный пайплайн (`.gitlab-ci.yml`).
3) При событии (push, merge request, tag) GitLab инициирует pipeline.
4) Pipeline выполняется на GitLab Runner:
   - SaaS runner (если допустимо политикой) и/или self-hosted runner на собственных серверах.
5) Сборка использует Docker-образ сборочного окружения (стандартный или корпоративный).
6) Секреты (пароли, ключи) не лежат в коде: подаются через переменные CI (masked/protected) и/или через Vault.
7) Результаты (артефакты, контейнерные образы) публикуются в Container Registry.
8) Доставка:
   - CD в Kubernetes/VM через деплой-джобы.
   - Разделение окружений (dev/stage/prod) через environments и правила.

### Как требования закрываются
- Облачная система: GitLab SaaS.
- Git и репозиторий на каждый сервис: отдельные Git-проекты.
- Запуск по событию: triggers по push/MR/tag.
- Запуск по кнопке с параметрами: manual jobs + pipeline variables.
- Настройки на уровне сборки: variables, rules, environments, protected branches.
- Шаблоны: includes (общие шаблоны пайплайнов) + YAML anchors/extends.
- Секреты: masked/protected variables, environment-level secrets, protected environments; опционально Vault.
- Несколько конфигураций из одного репо: несколько джоб/стейджей + rules/матрицы.
- Кастомные шаги: произвольные scripts в jobs.
- Собственные Docker-образы: `image:` в job + корпоративный registry.
- Self-hosted агенты: GitLab Runner на собственных серверах.
- Параллельные сборки: параллельные jobs/stages, несколько runners.
- Параллельные тесты: parallel jobs, test splitting, matrix.

### Обоснование выбора
- Единая платформа в облаке: SCM, CI/CD, registry, permissions и аудит в одном месте.
- Декларативные пайплайны как код: воспроизводимость и версионирование.
- Гибкое управление секретами и правами (protected branches/environments).
- Возможность вынести выполнение в собственную инфраструктуру (self-hosted runners) при сохранении облачного управления.
- Поддержка шаблонов и повторного использования пайплайнов для большого числа микросервисов.

---

## Задача 2: Логи (централизованный сбор и анализ)

### Требования
- Сбор логов в центральное хранилище со всех хостов.
- Минимальные требования к приложениям, сбор из stdout.
- Гарантированная доставка до центрального хранилища.
- Поиск и фильтрация по записям логов.
- UI с разграничением доступа для разработчиков.
- Возможность дать ссылку на сохранённый поиск.

### Предложенное решение
Стек:
- Сборщик на хостах: Fluent Bit (DaemonSet в Kubernetes или агент на VM).
- Буфер/гарантия доставки: Kafka (рекомендуется) или встроенный disk buffer (в зависимости от режима).
- Централизованное хранилище и поиск: Elasticsearch или OpenSearch.
- UI: Kibana или OpenSearch Dashboards.

Выбор (типовой для микросервисов с практичной эксплуатацией):
- Fluent Bit -> Kafka -> OpenSearch -> OpenSearch Dashboards.

### Как взаимодействуют компоненты
1) Приложения пишут логи в stdout/stderr (контейнерный runtime собирает их в файловые логи контейнеров).
2) Fluent Bit на каждом хосте читает контейнерные логи и добавляет метаданные (namespace, pod, container, service, env).
3) Fluent Bit отправляет события в Kafka (топики по окружениям/командам/сервисам).
4) Консьюмер (Fluent Bit output/Logstash/Data Prepper) читает из Kafka и пишет в OpenSearch индексами по времени и сервисам.
5) Разработчики ищут и фильтруют логи через Dashboards, сохраняют запросы и делятся ссылками.

### Как требования закрываются
- Центральное хранилище со всех хостов: агенты Fluent Bit на каждом узле.
- Минимальные требования к приложениям: stdout.
- Гарантированная доставка: Kafka как буфер и журнал доставки; при недоступности хранилища сообщения не теряются, потребление возобновляется.
- Поиск и фильтрация: индексация в OpenSearch.
- UI и доступы: Dashboards + RBAC (роли/индексы/тенанты) в OpenSearch.
- Ссылки на сохранённый поиск: saved searches/visualizations в Dashboards.

### Обоснование выбора
- Отделение приёма логов от хранилища через Kafka снижает риск потерь при сбоях OpenSearch.
- Fluent Bit лёгкий и подходит для контейнеров и VM, поддерживает парсинг и enrich.
- OpenSearch обеспечивает полнотекстовый поиск и фильтрацию, Dashboards даёт удобный UI и сохранённые запросы.

---

## Задача 3: Мониторинг (метрики и состояние)

### Требования
- Сбор метрик со всех хостов, обслуживающих систему.
- Метрики ресурсов хостов: CPU, RAM, HDD, Network.
- Метрики ресурсов каждого сервиса: CPU, RAM, HDD, Network.
- Специфичные метрики сервисов.
- UI с запросами и агрегациями.
- UI с настраиваемыми панелями.

### Предложенное решение
Стек:
- Сбор метрик: Prometheus.
- Метрики хостов: Node Exporter.
- Метрики контейнеров/подов: cAdvisor и/или kube-state-metrics (в Kubernetes).
- Метрики приложений: Prometheus client libraries (HTTP `/metrics`) и/или exporters (Postgres exporter, Redis exporter и т.д.).
- Долгое хранение/масштабирование (опционально): Thanos или VictoriaMetrics.
- UI и панели: Grafana.

Выбор (типовой):
- Prometheus + exporters + Grafana.

### Как взаимодействуют компоненты
1) Prometheus периодически опрашивает endpoints метрик.
2) Node Exporter отдаёт метрики ресурсов хоста.
3) Exporters и приложения отдают бизнес/технические метрики сервиса.
4) Prometheus хранит временные ряды и выполняет запросы PromQL.
5) Grafana подключается к Prometheus, строит панели и дашборды, позволяет агрегировать и фильтровать.
6) При необходимости длительного хранения и HA подключается Thanos/VictoriaMetrics.

### Как требования закрываются
- Все хосты: Node Exporter на каждом узле.
- CPU/RAM/HDD/Network хоста: метрики Node Exporter.
- Ресурсы каждого сервиса: метрики контейнеров (cAdvisor), метрики Kubernetes (kube-state-metrics) или process exporters на VM.
- Специфичные метрики: client libraries/exporters.
- UI для запросов и агрегаций: Grafana + PromQL.
- UI для панелей: Grafana dashboards.

### Обоснование выбора
- Prometheus является стандартом де-факто для метрик в микросервисах и контейнерных средах.
- Большая экосистема exporters и готовых дашбордов.
- Grafana предоставляет гибкий UI для построения панелей и работы с запросами.
- Стек расширяется до HA и long-term storage без смены подхода.

